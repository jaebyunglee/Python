{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow: GETTING STARTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is an open-source machine learning library for research and production. (https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual diagram of Tensorflow 2.0**\n",
    "\n",
    "![](https://miro.medium.com/max/700/0*fJ5u2WE51Oz44dr_)\n",
    "\n",
    "- Easy model building with `Keras` and `eager execution`.\n",
    "- Robust model deployment in production on any platform.\n",
    "- Powerful experimentation for research.\n",
    "- `Simplifying the API` by cleaning up deprecated APIs and reducing duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [참고] Eager execution?\n",
    "\n",
    "![](https://1.bp.blogspot.com/-CThvwj-LRq4/WuucqLXkYII/AAAAAAAAAlw/LYRt4pkuu4wnyw6BGV9H0bSIZ4HRmdJ2gCEwYBhgL/s1600/Screen%2BShot%2B2018-05-04%2Bat%2B8.30.40%2BAM.png)\n",
    "(출처: https://developers-kr.googleblog.com/2018/05/eager-execution.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tensorflow & Check the Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a constant Tensor\n",
    "hello = tf.constant(\"hello world\")\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access the Tensor value, call numpy().\n",
    "print(hello.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor constants.\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various tensor operations.\n",
    "# Note: Tensors also support python operators (+, *, ...)\n",
    "add = tf.add(a, b)\n",
    "sub = tf.subtract(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "div = tf.divide(a, b)\n",
    "\n",
    "# Access tensors value.\n",
    "print(\"add =\", add.numpy())\n",
    "print(\"sub =\", sub.numpy())\n",
    "print(\"mul =\", mul.numpy())\n",
    "print(\"div =\", div.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more operations.\n",
    "mean = tf.reduce_mean([a, b, c])\n",
    "sum_ = tf.reduce_sum([a, b, c])\n",
    "\n",
    "# Access tensors value.\n",
    "print(\"mean =\", mean.numpy())\n",
    "print(\"sum =\", sum_.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplications.\n",
    "matrix1 = tf.constant([[1., 2.], [3., 4.]])\n",
    "matrix2 = tf.constant([[5., 6.], [7., 8.]])\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Tensor.\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tensor to Numpy.\n",
    "product.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy와 Tensorflow의 데이터 배열\n",
    "- `tf.Tensor` vs. `np.ndarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.Tensor`는 numpy array와 달리 GPU (혹은 TPU)에서 사용할 수 있다.\n",
    "- 각각의 `tf.Tensor`는 크기와 데이터 타입을 가지고 있다.\n",
    "- `tf.Tensor`는 `.numpy()` 메서드를 통해 numpy array로 변환할 수 있다.\n",
    "  - 즉시 실행 (eager execution) 개념의 도입으로 두 데이터 타입 간 호환성이 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"텐서플로 연산은 자동적으로 넘파이 배열을 텐서로 변환합니다.\")\n",
    "tensor = tf.multiply(ndarray, 42)\n",
    "print(tensor)\n",
    "print()\n",
    "\n",
    "print('각각의 텐서는 크기와 데이터 타입을 가지고 있습니다.')\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print()\n",
    "\n",
    "print(\"그리고 넘파이 연산은 자동적으로 텐서를 넘파이 배열로 변환합니다.\")\n",
    "print(np.add(tensor, 1))\n",
    "print()\n",
    "\n",
    "print(\".numpy() 메서드는 텐서를 넘파이 배열로 변환합니다.\")\n",
    "print(tensor.numpy())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU를 사용한 연산 가속화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서플로는 연산을 위해 자동적으로 CPU 또는 GPU를 사용할 것인지를 정합니다.\n",
    "- 그리고 필요시 텐서를 CPU 와 GPU에 복사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([3, 3])\n",
    "\n",
    "print(\"GPU 사용이 가능한가 : \"),\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "print(\"텐서가 GPU #0에 있는가 :  \"),\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주어진 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data\n",
    "train_x = [1,2,3,4]\n",
    "train_y = [0,-1,-2,-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 및 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias: initialized with fixed values.\n",
    "W = tf.Variable([.3], dtype=tf.float32, name=\"weight\")\n",
    "b = tf.Variable([-.3], dtype=tf.float32, name=\"bias\")\n",
    "\n",
    "# Weight and Bias: initialized randomly.\n",
    "# W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "# b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# a simple linear model\n",
    "def linear_model(x):\n",
    "    return x * W + b\n",
    "\n",
    "# Mean square error.\n",
    "def mean_square_loss(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.pow(y_pred-y_true, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 현재 모델의 예측값 및 손실함수값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all at once.\n",
    "yhat = linear_model(train_x)\n",
    "loss = mean_square_loss(yhat, train_y)\n",
    "print('input:  ', train_x)\n",
    "print('y_true: ', train_y)\n",
    "print('y_pred: ', yhat.numpy())\n",
    "print('loss:   ', loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute separately.\n",
    "for x, y in zip(train_x, train_y):\n",
    "    yhat = linear_model(x)\n",
    "    loss = mean_square_loss(yhat, y)\n",
    "    print('input:  ', x)\n",
    "    print('y_true: ', y)\n",
    "    print('y_pred: ', yhat.numpy())\n",
    "    print('loss:   ', loss.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 손으로 최적해 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# data visualization\n",
    "plt.scatter(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixW = W.assign([-1.])\n",
    "fixb = b.assign([1.])\n",
    "\n",
    "def solution_model(x):\n",
    "    return x * fixW + fixb\n",
    "\n",
    "yhat = solution_model(train_x)\n",
    "loss = mean_square_loss(yhat, train_y)\n",
    "\n",
    "print('input:  ', train_x)\n",
    "print('y_true: ', train_y)\n",
    "print('y_pred: ', yhat.numpy())\n",
    "print('loss:   ', loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그렇지만, 차원이 크고 관계가 복잡하다면...?\n",
    "\n",
    "#### 데이터를 잘 설명하는 최적의 w, b를 찾기 위한 학습 과정이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient descent optimizer.\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_model(x)\n",
    "        loss = mean_square_loss(pred, y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    trainable_variables = [W, b]\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(100):\n",
    "    for x, y in zip(train_x, train_y):\n",
    "        # Run the optimization to update W and b values.\n",
    "        run_optimization(x, y)\n",
    "\n",
    "    yhat = linear_model(train_x)\n",
    "    loss = mean_square_loss(yhat, train_y)\n",
    "    print(\"step: %i, loss: %f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = linear_model(train_x)\n",
    "loss = mean_square_loss(yhat, train_y)\n",
    "print('input:  ', train_x)\n",
    "print('y_true: ', train_y)\n",
    "print('y_pred: ', yhat.numpy())\n",
    "print('loss:   ', loss.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
