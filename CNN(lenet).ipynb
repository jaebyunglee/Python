{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert from unit8 to floate32 and\n",
    "- normalize images value from[0,255] to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)/255\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add new axis for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.reshape(x_train,[-1,28,28,1])\n",
    "x_test = tf.reshape(x_test,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  156       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  2416      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  30840     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "lenet = Sequential([\n",
    "        #input 28 * 28 * 1 #output = 24*24*6\n",
    "        layers.Conv2D(6,(5,5), padding=\"VALID\"),\n",
    "        layers.Activation(\"relu\"),\n",
    "        #polling layer\n",
    "        #input 24*24*6 #output 12*12*6\n",
    "        layers.MaxPooling2D(pool_size = (2,2), strides=2),\n",
    "        #convlolutional Layer\n",
    "        #input 12*12*6 #output 8*8*16\n",
    "        layers.Conv2D(16,(5,5), padding=\"VALID\"),\n",
    "        layers.Activation(\"relu\"),\n",
    "        #polling Layer\n",
    "        #input 8*8*16 #output 4*4*16\n",
    "        layers.MaxPooling2D(pool_size = (2,2), strides=2),\n",
    "        #flatten layer\n",
    "        # input 4*4*16 output =256\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation=\"relu\"),\n",
    "        layers.Dense(84, activation=\"relu\"),\n",
    "        layers.Dense(10)])\n",
    "\n",
    "lenet.build(input_shape=(None,28,28,1))\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, \n",
    "                                                          logits=y_pred)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500, loss: 0.090076, accuracy: 0.972400\n",
      "step: 1000, loss: 0.059174, accuracy: 0.981200\n",
      "step: 1500, loss: 0.050391, accuracy: 0.984000\n",
      "step: 2000, loss: 0.047117, accuracy: 0.984600\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Model Training\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Use tf.data API to batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "training_batch = train_data.batch(batch_size).repeat(epochs)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# Training parameters.\n",
    "display_step = 500\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(training_batch, 1):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = lenet(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    trainable_variables = lenet.trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = lenet(x_test)\n",
    "        loss = cross_entropy(pred, y_test)\n",
    "        acc = accuracy(pred, y_test)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
