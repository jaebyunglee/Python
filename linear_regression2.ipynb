{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 error를 최소화 하는 베타를 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300; p = 10\n",
    "tf.compat.v1.random.set_random_seed(1234)\n",
    "#b_vec = tf.constant([range(10,0,-1)])\n",
    "#b_vec = tf.divide(b_vec,p).numpy()\n",
    "x_mat = tf.random.normal([n,p]).numpy()\n",
    "y_vec = tf.random.normal([n,1]).numpy()\n",
    "#y_vec = np.matmul(x_mat,np.transpose(b_vec)) + tf.random.normal([n,1]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_x_mat = x_mat[0:200,]\n",
    "train_y_vec = y_vec[0:200]\n",
    "test_x_mat = x_mat[200:301,]\n",
    "test_y_vec = y_vec[200:301]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform tf\n",
    "train_x_mat = tf.constant(train_x_mat,dtype=tf.float32)\n",
    "train_y_vec = tf.constant(train_y_vec,dtype=tf.float32)\n",
    "test_x_mat = tf.constant(test_x_mat,dtype=tf.float32)\n",
    "test_y_vec = tf.constant(test_y_vec,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W ,b 초기값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias: initialized randomly.\n",
    "tf.compat.v1.random.set_random_seed(1)\n",
    "W = tf.Variable(tf.random.normal([p, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형회귀, 로스함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x):\n",
    "    return tf.matmul(x,W) + b\n",
    "    \n",
    "# Mean square error.\n",
    "def mean_square_loss(y_pred,y_true):\n",
    "    return tf.reduce_mean(tf.pow(y_pred-y_true,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 시키기 \n",
    "- (200 epoch 해본 후, 135 epoch이 최적임을 알아냄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 10.2703\n",
      "epoch: 2, loss: 9.5241\n",
      "epoch: 3, loss: 8.8400\n",
      "epoch: 4, loss: 8.2129\n",
      "epoch: 5, loss: 7.6376\n",
      "epoch: 6, loss: 7.1099\n",
      "epoch: 7, loss: 6.6256\n",
      "epoch: 8, loss: 6.1811\n",
      "epoch: 9, loss: 5.7730\n",
      "epoch: 10, loss: 5.3982\n",
      "epoch: 11, loss: 5.0538\n",
      "epoch: 12, loss: 4.7374\n",
      "epoch: 13, loss: 4.4466\n",
      "epoch: 14, loss: 4.1792\n",
      "epoch: 15, loss: 3.9332\n",
      "epoch: 16, loss: 3.7070\n",
      "epoch: 17, loss: 3.4989\n",
      "epoch: 18, loss: 3.3073\n",
      "epoch: 19, loss: 3.1310\n",
      "epoch: 20, loss: 2.9686\n",
      "epoch: 21, loss: 2.8190\n",
      "epoch: 22, loss: 2.6811\n",
      "epoch: 23, loss: 2.5541\n",
      "epoch: 24, loss: 2.4371\n",
      "epoch: 25, loss: 2.3291\n",
      "epoch: 26, loss: 2.2296\n",
      "epoch: 27, loss: 2.1377\n",
      "epoch: 28, loss: 2.0530\n",
      "epoch: 29, loss: 1.9748\n",
      "epoch: 30, loss: 1.9026\n",
      "epoch: 31, loss: 1.8360\n",
      "epoch: 32, loss: 1.7744\n",
      "epoch: 33, loss: 1.7175\n",
      "epoch: 34, loss: 1.6650\n",
      "epoch: 35, loss: 1.6165\n",
      "epoch: 36, loss: 1.5716\n",
      "epoch: 37, loss: 1.5301\n",
      "epoch: 38, loss: 1.4917\n",
      "epoch: 39, loss: 1.4562\n",
      "epoch: 40, loss: 1.4234\n",
      "epoch: 41, loss: 1.3930\n",
      "epoch: 42, loss: 1.3649\n",
      "epoch: 43, loss: 1.3389\n",
      "epoch: 44, loss: 1.3148\n",
      "epoch: 45, loss: 1.2924\n",
      "epoch: 46, loss: 1.2718\n",
      "epoch: 47, loss: 1.2526\n",
      "epoch: 48, loss: 1.2349\n",
      "epoch: 49, loss: 1.2185\n",
      "epoch: 50, loss: 1.2032\n",
      "epoch: 51, loss: 1.1891\n",
      "epoch: 52, loss: 1.1760\n",
      "epoch: 53, loss: 1.1639\n",
      "epoch: 54, loss: 1.1527\n",
      "epoch: 55, loss: 1.1422\n",
      "epoch: 56, loss: 1.1326\n",
      "epoch: 57, loss: 1.1236\n",
      "epoch: 58, loss: 1.1152\n",
      "epoch: 59, loss: 1.1075\n",
      "epoch: 60, loss: 1.1003\n",
      "epoch: 61, loss: 1.0937\n",
      "epoch: 62, loss: 1.0875\n",
      "epoch: 63, loss: 1.0818\n",
      "epoch: 64, loss: 1.0765\n",
      "epoch: 65, loss: 1.0715\n",
      "epoch: 66, loss: 1.0669\n",
      "epoch: 67, loss: 1.0627\n",
      "epoch: 68, loss: 1.0587\n",
      "epoch: 69, loss: 1.0550\n",
      "epoch: 70, loss: 1.0516\n",
      "epoch: 71, loss: 1.0484\n",
      "epoch: 72, loss: 1.0455\n",
      "epoch: 73, loss: 1.0428\n",
      "epoch: 74, loss: 1.0402\n",
      "epoch: 75, loss: 1.0379\n",
      "epoch: 76, loss: 1.0357\n",
      "epoch: 77, loss: 1.0336\n",
      "epoch: 78, loss: 1.0317\n",
      "epoch: 79, loss: 1.0300\n",
      "epoch: 80, loss: 1.0283\n",
      "epoch: 81, loss: 1.0268\n",
      "epoch: 82, loss: 1.0254\n",
      "epoch: 83, loss: 1.0241\n",
      "epoch: 84, loss: 1.0229\n",
      "epoch: 85, loss: 1.0218\n",
      "epoch: 86, loss: 1.0207\n",
      "epoch: 87, loss: 1.0197\n",
      "epoch: 88, loss: 1.0188\n",
      "epoch: 89, loss: 1.0180\n",
      "epoch: 90, loss: 1.0172\n",
      "epoch: 91, loss: 1.0165\n",
      "epoch: 92, loss: 1.0158\n",
      "epoch: 93, loss: 1.0152\n",
      "epoch: 94, loss: 1.0146\n",
      "epoch: 95, loss: 1.0141\n",
      "epoch: 96, loss: 1.0136\n",
      "epoch: 97, loss: 1.0131\n",
      "epoch: 98, loss: 1.0127\n",
      "epoch: 99, loss: 1.0123\n",
      "epoch: 100, loss: 1.0119\n",
      "epoch: 101, loss: 1.0116\n",
      "epoch: 102, loss: 1.0113\n",
      "epoch: 103, loss: 1.0110\n",
      "epoch: 104, loss: 1.0107\n",
      "epoch: 105, loss: 1.0105\n",
      "epoch: 106, loss: 1.0102\n",
      "epoch: 107, loss: 1.0100\n",
      "epoch: 108, loss: 1.0098\n",
      "epoch: 109, loss: 1.0097\n",
      "epoch: 110, loss: 1.0095\n",
      "epoch: 111, loss: 1.0093\n",
      "epoch: 112, loss: 1.0092\n",
      "epoch: 113, loss: 1.0091\n",
      "epoch: 114, loss: 1.0089\n",
      "epoch: 115, loss: 1.0088\n",
      "epoch: 116, loss: 1.0087\n",
      "epoch: 117, loss: 1.0086\n",
      "epoch: 118, loss: 1.0085\n",
      "epoch: 119, loss: 1.0085\n",
      "epoch: 120, loss: 1.0084\n",
      "epoch: 121, loss: 1.0083\n",
      "epoch: 122, loss: 1.0083\n",
      "epoch: 123, loss: 1.0082\n",
      "epoch: 124, loss: 1.0082\n",
      "epoch: 125, loss: 1.0081\n",
      "epoch: 126, loss: 1.0081\n",
      "epoch: 127, loss: 1.0080\n",
      "epoch: 128, loss: 1.0080\n",
      "epoch: 129, loss: 1.0080\n",
      "epoch: 130, loss: 1.0079\n",
      "epoch: 131, loss: 1.0079\n",
      "epoch: 132, loss: 1.0079\n",
      "epoch: 133, loss: 1.0079\n",
      "epoch: 134, loss: 1.0079\n",
      "epoch: 135, loss: 1.0078\n"
     ]
    }
   ],
   "source": [
    "#test로스를 최소화 하는 w,b 찾기\n",
    "#epochs = 200\n",
    "epochs = 135\n",
    "batch_size = 5\n",
    "learning_rate = 0.0005\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_x_mat,train_y_vec))\n",
    "training_batch = train_data.batch(batch_size).repeat(epochs)\n",
    "\n",
    "\n",
    "# Interval for showing the training status.\n",
    "display_step = len(train_y_vec)/batch_size\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(training_batch, 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_model(batch_x)\n",
    "        loss = mean_square_loss(pred, batch_y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    trainable_variables = [W, b]\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = linear_model(test_x_mat)\n",
    "        loss = mean_square_loss(pred, test_y_vec)\n",
    "        print(\"epoch: %i, loss: %.4f\" % (step / display_step, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습된 W와 b 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 1.0078435 \n",
      "\n",
      "b: [0.04962714] \n",
      "\n",
      "W: [[ 0.08158664]\n",
      " [-0.04445674]\n",
      " [ 0.03016776]\n",
      " [ 0.00207449]\n",
      " [ 0.02357498]\n",
      " [ 0.1417609 ]\n",
      " [ 0.000469  ]\n",
      " [-0.17816864]\n",
      " [ 0.05539262]\n",
      " [ 0.06723822]]\n"
     ]
    }
   ],
   "source": [
    "print('test error:',loss.numpy(),'\\n')\n",
    "print('b:',b.numpy(),'\\n')\n",
    "print('W:',W.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 1.0081856 \n",
      "\n",
      "b: [0.04286529] \n",
      "\n",
      "W: [[ 0.08542245 -0.06130634  0.02489171  0.0090178   0.03340093  0.14455536\n",
      "  -0.0048029  -0.18924932  0.06529539  0.06921971]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# sklearn package\n",
    "lrm = LinearRegression(n_jobs=-1)   \n",
    "lrm.fit(train_x_mat,train_y_vec)\n",
    "print(\"test error:\",mean_square_loss(lrm.predict(test_x_mat),test_y_vec).numpy(),\"\\n\")\n",
    "print(\"b:\",lrm.intercept_ ,\"\\n\")\n",
    "print(\"W:\", lrm.coef_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
