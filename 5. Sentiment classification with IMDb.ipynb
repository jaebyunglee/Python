{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb dataset for sentiment classification\n",
    "\n",
    "- 영화 평가 사이트 IMDb의 리뷰를 바탕으로 만들어짐 (http://www.imdb.com/interfaces/)\n",
    "- sentiment classification을 위한 대표적인 벤치마크 데이터\n",
    "- training data와 test data 각각 25,000개의 리뷰가 담겨 있음\n",
    "\n",
    "![](https://iksinc.files.wordpress.com/2015/09/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:   25000 samples\n",
      "Test Set:       3913 samples\n",
      "\n",
      "An example of sequence and its label:\n",
      "- sequence: [1, 14, 9, 31, 7, 148, 102, 198, 269, 8, 30, 4378, 5, 3094, 5, 305, 630, 56, 2, 32, 120, 410, 260, 110, 12, 33, 6, 2, 22, 1413, 13, 16, 3704, 34, 4, 185, 1170, 2, 825, 355, 901, 56, 190, 120, 32, 1054, 56, 179, 685, 10, 10, 45, 254, 8, 6167, 6, 283, 65, 237, 225, 24, 76, 15, 70, 30, 224, 44, 4, 114, 21, 13, 258, 14, 4229, 3650, 5, 5028, 2279, 45, 465, 5, 220, 2950, 3370, 6, 5503, 948, 3174, 7, 4, 4039, 19, 2, 228, 5, 2, 491, 1969, 12, 43, 152, 157, 49, 139, 121, 38, 954, 15, 305, 7, 2, 4299, 61, 311, 16, 2, 2, 5, 2660, 523, 10, 10, 4, 65, 47, 35, 221, 863, 21, 14, 43, 2, 2, 83, 6, 465, 4309, 7941]\n",
      "- label: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "maxlen = 200\n",
    "num_words = 8000\n",
    "\n",
    "(X_trn, y_trn), (X_tst, y_tst) = imdb.load_data(\n",
    "    path='imdb.pkl',\n",
    "    num_words=num_words,\n",
    "    skip_top=0,\n",
    "    maxlen=maxlen,\n",
    "    seed=0, #113\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3)\n",
    "\n",
    "print(\"Training Set:   {} samples\".format(len(X_trn)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_tst)))\n",
    "print()\n",
    "print(\"An example of sequence and its label:\")\n",
    "print('- sequence:', X_trn[0])\n",
    "print('- label:', y_trn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [참고] word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this is one of those movies that's trying to be moody and tense and instead ends up <UNK> all over itself having seen it at a <UNK> film festival i was intrigued by the young college <UNK> gone wrong write up however over all ended up quite disappointed br br it's hard to critique a true story since there's not much that can be done about the plot but i found this disjointed melodramatic and wholly depressing it's dark and almost sinister painting a darn creepy flash of the seventies with <UNK> music and <UNK> close ups it just doesn't work some scenes where so cheesy that instead of <UNK> awe my audience was <UNK> <UNK> and rolling eyes br br the story has an interesting premise but this just <UNK> <UNK> into a dark miserable spiral\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "print(' '.join(id_to_word[i] for i in X_trn[0]))\n",
    "print(y_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set에서 보지 못한 단어를 test set으로부터 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size 8000\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary_size(X):\n",
    "    \"\"\"\n",
    "    input (X): [num_document, document_size (variable_length)]\n",
    "    output   : vocabulary size\n",
    "    \"\"\"\n",
    "    return max([max(doc) for doc in X]) + 1  # plus the 0th word\n",
    "\n",
    "def fit_in_vocabulary(X, voc_size):\n",
    "    \"\"\"\n",
    "    convert the index of OOV (out-of-vocabulary) word to \"2\"\n",
    "    input (X): [num_document, document_size (variable_length)]\n",
    "    output   : [num_document, document_size (variable_length)]\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(X):\n",
    "        X[i] = [w if w < voc_size else 2 for w in doc]\n",
    "    return X\n",
    "\n",
    "vocabulary_size = get_vocabulary_size(X_trn)\n",
    "X_tst = fit_in_vocabulary(X_tst, vocabulary_size)\n",
    "print('vocabulary_size', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero-padding: 모든 sequence의 길이를 동일하게 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (3913, 200)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_trn = pad_sequences(X_trn, maxlen=maxlen, padding='post')\n",
    "X_tst = pad_sequences(X_tst, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_trn.shape)\n",
    "print('x_test shape:', X_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((X_trn, y_trn))\n",
    "                 .shuffle(len(X_trn)).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((X_tst, y_tst))\n",
    "                .batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://image.slidesharecdn.com/l07nnrnngrulstm-151108140716-lva1-app6892/95/recurrent-neural-networks-lstm-and-gru-14-638.jpg?cb=1446992496)\n",
    "\n",
    "We need many-to-one structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://deeplearning.net/tutorial/_images/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "# Network Parameters\n",
    "num_input = num_words # number of sequences.\n",
    "timesteps = maxlen    # timesteps.\n",
    "embedding_dim = 16 # embedding dimention of one-hot vectors\n",
    "num_units = 32 # number of neurons for the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM Model.\n",
    "class LSTM(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding_layer = layers.Embedding(input_dim=vocabulary_size, \n",
    "                                                output_dim=embedding_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.lstm_layer = layers.LSTM(units=num_units, \n",
    "                                      return_sequences=True,\n",
    "                                      zero_output_for_mask=True)\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        # Embedding layer\n",
    "        # input=[batch_size, maxlen, vocabulary_size]\n",
    "        # output=[batch_size, maxlen, embedding_dim]\n",
    "        x_embedded = self.embedding_layer(x)\n",
    "        \n",
    "        # mask for zero-padded parts\n",
    "        mask = self.embedding_layer.compute_mask(x)\n",
    "        \n",
    "        # LSTM layer.\n",
    "        # input=[batch_size, maxlen, embedding_dim]\n",
    "        # output=[batch_size, maxlen, num_units]\n",
    "        rnn_outputs = self.lstm_layer(x_embedded, mask=mask)\n",
    "        \n",
    "        # average outputs over time axis\n",
    "        # input=[batch_size, maxlen, num_units]\n",
    "        # output=[batch_size, num_units]\n",
    "        casted_mask = tf.cast(mask, tf.float32)\n",
    "        masked_outputs = tf.multiply(rnn_outputs, \n",
    "                                     tf.expand_dims(casted_mask, 2))\n",
    "        lengths = tf.reduce_sum(casted_mask, axis=1)\n",
    "        average = tf.divide(tf.reduce_sum(masked_outputs, axis=1), \n",
    "                            tf.expand_dims(lengths, 1))\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.out(average)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 39s 400ms/step - loss: 0.5689 - accuracy: 0.5823 - val_loss: 0.3949 - val_accuracy: 0.8423\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 32s 326ms/step - loss: 0.3352 - accuracy: 0.8723 - val_loss: 0.3337 - val_accuracy: 0.8674\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 32s 323ms/step - loss: 0.2496 - accuracy: 0.9065 - val_loss: 0.3257 - val_accuracy: 0.8666\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 32s 325ms/step - loss: 0.2136 - accuracy: 0.9242 - val_loss: 0.3392 - val_accuracy: 0.8617\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 32s 326ms/step - loss: 0.1923 - accuracy: 0.9358 - val_loss: 0.3327 - val_accuracy: 0.8623\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 32s 327ms/step - loss: 0.1812 - accuracy: 0.9412 - val_loss: 0.3136 - val_accuracy: 0.8633\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 32s 325ms/step - loss: 0.1757 - accuracy: 0.9439 - val_loss: 0.3279 - val_accuracy: 0.8610\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 31s 314ms/step - loss: 0.1686 - accuracy: 0.9458 - val_loss: 0.3743 - val_accuracy: 0.8589\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 32s 325ms/step - loss: 0.1506 - accuracy: 0.9518 - val_loss: 0.4750 - val_accuracy: 0.8505\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 32s 325ms/step - loss: 0.1445 - accuracy: 0.9536 - val_loss: 0.4451 - val_accuracy: 0.8533\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.4451 - accuracy: 0.8533\n",
      "Test Loss: 0.4450914040207863\n",
      "Test Accuracy: 0.8533094525337219\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "model = LSTM()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "                    validation_data=test_dataset)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM Model.\n",
    "class BidirectionalLSTM(LSTM):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.lstm_layer = layers.Bidirectional(self.lstm_layer, merge_mode='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 86s 880ms/step - loss: 0.5022 - accuracy: 0.6291 - val_loss: 0.3009 - val_accuracy: 0.8796\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 74s 753ms/step - loss: 0.2460 - accuracy: 0.8964 - val_loss: 0.2590 - val_accuracy: 0.8985\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 78s 799ms/step - loss: 0.2020 - accuracy: 0.9262 - val_loss: 0.2737 - val_accuracy: 0.8975\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 89s 912ms/step - loss: 0.1738 - accuracy: 0.9397 - val_loss: 0.2849 - val_accuracy: 0.8893\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 82s 838ms/step - loss: 0.1508 - accuracy: 0.9459 - val_loss: 0.3065 - val_accuracy: 0.8860\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 80s 818ms/step - loss: 0.1439 - accuracy: 0.9447 - val_loss: 0.3619 - val_accuracy: 0.8758\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 77s 783ms/step - loss: 0.1342 - accuracy: 0.9457 - val_loss: 0.3565 - val_accuracy: 0.8832\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 85s 864ms/step - loss: 0.1227 - accuracy: 0.9541 - val_loss: 0.3621 - val_accuracy: 0.8837\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 86s 875ms/step - loss: 0.1082 - accuracy: 0.9657 - val_loss: 0.3497 - val_accuracy: 0.8789\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 86s 875ms/step - loss: 0.1038 - accuracy: 0.9712 - val_loss: 0.3809 - val_accuracy: 0.8870\n",
      "16/16 [==============================] - 10s 649ms/step - loss: 0.3809 - accuracy: 0.8870\n",
      "Test Loss: 0.3809265084564686\n",
      "Test Accuracy: 0.8870431780815125\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "model = BidirectionalLSTM()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "                    validation_data=test_dataset)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the sentiment of your review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally predictable but good for weekend\n",
      "==> sentment score [0.31] -- bad movie!\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_score(review_text):\n",
    "    review_seq = review_text.split(' ')\n",
    "    data = [[word_to_id[w] for w in review_seq]]\n",
    "    data = fit_in_vocabulary(data, vocabulary_size)\n",
    "\n",
    "    padded_data = pad_sequences(data, maxlen=maxlen, padding='post')\n",
    "    y = model.predict(padded_data)\n",
    "    return y.flatten()\n",
    "\n",
    "review_text = 'totally predictable but good for weekend'\n",
    "# review_text = 'how lovely the actress'\n",
    "# review_text = 'good soundtrack'\n",
    "# review_text = 'good ost'\n",
    "# review_text = 'horrible'\n",
    "\n",
    "y = get_sentiment_score(review_text)\n",
    "print(review_text)\n",
    "print('==> sentment score', y.round(2), '--',\n",
    "      'good' if y > 0.5 else 'bad', 'movie!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
