{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we will first load the MNIST dataset.\n",
    "\n",
    "To use 2d convolutional filters, we do not flatten (reshape) the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert from uint8 to float32 and\n",
    "# normalize images value from [0, 255] to [0, 1].\n",
    "# (uint8: Unsigned integer (0 to 255))\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) / 255.\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\n",
    "\n",
    "# add new axis for channel\n",
    "x_train = tf.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = tf.reshape(x_test, [-1, 28, 28, 1])\n",
    "\n",
    "print(\"Image Shape: {}\".format(x_train[0].shape))\n",
    "print(\"Label Shape: {}\".format(y_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(x_train)))\n",
    "print(\"Test Set:       {} samples\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Convolutional Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LeNet-5 with Convolution and Pooling operations\n",
    "\n",
    "![](https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/lenet-5-825x285.png?x64257)\n",
    "\n",
    "- conv2d: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d\n",
    "- max_pool2d: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = Sequential([\n",
    "    # convolutional layer\n",
    "    # input = 28x28x1, output = 24x24x6\n",
    "    layers.Conv2D(6, (5, 5), padding='VALID'),\n",
    "    layers.Activation('relu'),\n",
    "    # pooling layer\n",
    "    # input = 24x24x6, output = 12x12x6\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    # convolutional layer\n",
    "    # input = 12x12x6, output = 8x8x16\n",
    "    layers.Conv2D(16, (5, 5), padding='VALID'),\n",
    "    layers.Activation('relu'),\n",
    "    # pooling layer\n",
    "    # input = 8x8x16, output = 4x4x16\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    # flatten layer\n",
    "    # input = 4x4x16, output = 256\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10)])\n",
    "# [batch_size, image_width, image_height, channels]\n",
    "lenet.build(input_shape=(None, 28, 28, 1))\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, \n",
    "                                                          logits=y_pred)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Training\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "training_steps = 10\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "training_batch = train_data.batch(batch_size).repeat(training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Metric: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "display_step = 500\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(training_batch, 1):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = lenet(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    trainable_variables = lenet.trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = lenet(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on validation set.\n",
    "pred = lenet(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 5 images from validation set.\n",
    "n_images = 5\n",
    "test_images = x_test[:n_images]\n",
    "predictions = lenet(test_images)\n",
    "\n",
    "# Display image and model prediction.\n",
    "for i in range(n_images):\n",
    "    title = \"Model prediction: %i\" % np.argmax(predictions.numpy()[i])\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
